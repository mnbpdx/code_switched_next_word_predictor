{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5PrdcMI6RPuccf9dzC4ZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnbpdx/code_switched_next_word_predictor/blob/main/synonym_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps, from [Refined Project Proposal](https://docs.google.com/document/d/1NRUdfsiXkgPQW7Mg-CSVs8FDzghQjYWcvZ9JPUFfLW4/edit?usp=sharing)\n",
        "\n",
        "1. Pass the predicted next word to a pre-trained language recognition model to **determine the language** of the word (one of the two languages in our mixed, code-switched corpus.)\n",
        "2. **Translate** the predicted next word into the other language by passing it into an appropriate translation model.\n",
        "3. **Get n synonyms** of predicted next word in both languages using cosine distance between word embeddings, gathered from two vector embedding models, one in each language. This could also be done by a GPT-3 model.\n",
        "4. **Score the model** based on whether or not the actual next word is in the list of predicted next word bilingual synonyms.\n"
      ],
      "metadata": {
        "id": "leY02gxZwIZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Synonym Pipeline"
      ],
      "metadata": {
        "id": "QR8zDeBpu6vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports/Downloads"
      ],
      "metadata": {
        "id": "vSoAcqdvvZjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U easynmt\n",
        "!pip install sacremoses # was told to install this by a warning\n",
        "\n",
        "from easynmt import EasyNMT\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "QFXklFVVvbl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a802b446-ad43-4b3d-ae2b-20af9814e5bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: easynmt in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.12.1+cu113)\n",
            "Requirement already satisfied: transformers<5,>=4.4 in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.19.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from easynmt) (0.1.97)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (from easynmt) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->easynmt) (4.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2022.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5,>=4.4->easynmt) (3.0.9)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (2.10.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=4.4->easynmt) (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fake Data"
      ],
      "metadata": {
        "id": "_WOsNzR3viK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy Data\n",
        "predictions = ['pizza', 'turkey', 'baño', 'book', 'manzana', 'tiger']\n",
        "actual =    ['whale', 'chicken', 'bathroom', 'libro', 'plátano', 'tiger']"
      ],
      "metadata": {
        "id": "m1BrAIG8u95-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "tUbm1Gyf4SGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I think I should be raising the exceptions here and catching them in\n",
        "# the pipeline function actually, but i dont wanna waste time on that rn. \n",
        "def langs_are_valid(lang1, lang2):\n",
        "  if lang1 == lang2:\n",
        "    print('Langugages must not be the same.')\n",
        "    return False\n",
        "  \n",
        "  if lang1 != 'en' and lang1 != 'es':\n",
        "    print('Langugage must be either Spanish or English.')\n",
        "    return False\n",
        "  \n",
        "  if lang2 != 'en' and lang2 != 'es':\n",
        "    print('Langugage must be either Spanish or English.')\n",
        "    return False\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "Ba-4kOKv4T6P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "IwC7UnjtvjY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "from nltk.corpus.reader import wordlist\n",
        "# -- CITE ME -- Look at Mark's scratchpad noteboook for everything we gotta cite\n",
        "# -- POSSIBLE ISSUE -- 2x check my old comments for issues\n",
        "# -- POSSIBLE ISSUE -- -- DISCUSSION -- When I run id_model.language_detection on \"manzana\" it returns 'en'.\n",
        "# \"manzana\" IS a word in English, that means like a unit of land or something. That\n",
        "# wouldn't be a problem (hey, maybe the person speaking Spanglish here DID mean \"a unit of land\")\n",
        "# but in this case, it's SO obvious that there's another, much more common Spanish word\n",
        "# that means \"manzana\". I feel like the ID model should have caught that, by comparing\n",
        "# the frequency of the English and Spanish versions. My guess is that this model is\n",
        "# more used to translating sentences in context rather than words. We can sub it out\n",
        "# with a different language detection/identification model later then, if we notice this problem at a large scale.\n",
        "# That shouldn't be hard, there's many. I also wonder if what's happening here is that\n",
        "# the model is capable of interpreting semi-code-switched sentences and translating them.\n",
        "# It has to ID them first, so it would interpret an English sentence with a few Spanish words\n",
        "# as \"English\", then do the translation step. For that to work, it'd have to have some sense\n",
        "# of \"manzana\" as an English word. IOW, could be a NN problem, or a problem of a model that\n",
        "# is intended to do translation too. Same easy solution though, just sub it out. We\n",
        "# could even sub it out with an API call to Dictionary.com or something, doesn't\n",
        "# have to be a NN model.\n",
        "# Okay now I'm getting different languages detected than English or Spanish. Which\n",
        "# is a tougher problem to solve. We can switch out the model, but idk if I'm gonna\n",
        "# be able to find a model that allows us to specify: \"Only return one of these\n",
        "# two languages\". The big question is, what do we when our detection model returns\n",
        "# null, or returns an unknown language? Throw out the prediction from the score?\n",
        "\n",
        "\n",
        "# Right now, this function only ACTUALLY works for Spanish and English, gotta\n",
        "# figure out how to convert from the EasyNMT language strings to the WordNet\n",
        "# strings. Or I could just hack it. I'm not 100% sure yet that WN works for \n",
        "# Mandarin anyway, which would be an issue.\n",
        "def full_synonym_pipeline_rename_me(predictions, actual, lang1, lang2):\n",
        "  if not langs_are_valid(lang1, lang2):\n",
        "    raise Exception('Please use valid EasyNMT language strings.') # idk how to use exceptions in python, someone tell me if this is wrong\n",
        "\n",
        "  id_model = EasyNMT('opus-mt')\n",
        "  translation_model = EasyNMT('opus-mt')\n",
        "\n",
        "  synonym_lists = []\n",
        "  scores = []\n",
        "\n",
        "  for word in predictions:\n",
        "\n",
        "    # Determine the language of word\n",
        "    detected_lang = id_model.language_detection(word)\n",
        "    if (detected_lang == lang1):\n",
        "      other_lang = lang2\n",
        "    elif (detected_lang == lang2): \n",
        "      other_lang = lang1\n",
        "    else:\n",
        "      print('EasyNMT detected a language other than lang1 or lang2. Detected Language -> ' + detected_lang)\n",
        "      raise Exception\n",
        "\n",
        "    # Translate word into the OTHER language\n",
        "    translated_word = translation_model.translate(word, source_lang=detected_lang, target_lang=other_lang) # or swap the languages for the reverse, obv. \n",
        "\n",
        "    if (detected_lang == 'en'): # need to update this step to make this function generic with respect to languages\n",
        "      detected_lang = 'eng'\n",
        "      other_lang = 'spa'\n",
        "    else:\n",
        "      detected_lang = 'spa'\n",
        "      other_lang = 'eng'\n",
        "\n",
        "    # -- POSSIBLE ISSUE -- Should probably limit the number of words, right now there's roughly 0 - 20 for each.\n",
        "    # Get synonym lists for word and translated_word\n",
        "\n",
        "    # -- POSSIBLE ISSUE -- I'm combining the word and translated_word synonyms, we can split later if needed\n",
        "    synonyms = []\n",
        "\n",
        "    # add the words themselves to the synonym list\n",
        "    synonyms.append(word)\n",
        "    synonyms.append(translated_word)\n",
        "\n",
        "    # add synonyms of word\n",
        "    for synonym in wn.synsets(word, lang=detected_lang):\n",
        "      for item in synonym.lemmas(detected_lang):\n",
        "          if item.name() != word:\n",
        "            synonyms.append(item.name())\n",
        "\n",
        "    # add synonyms of translated_word\n",
        "    for synonym in wn.synsets(translated_word, lang=other_lang):\n",
        "      for item in synonym.lemmas(other_lang):\n",
        "          if item.name() != translated_word:\n",
        "            synonyms.append(item.name())\n",
        "\n",
        "    # Throw all the discovered synonyms (and word & translated_word) onto synonym_lists\n",
        "    synonym_lists.append(synonyms)\n",
        "\n",
        "  print()\n",
        "  print()\n",
        "  print(\"PREDICTIONS: \" + str(predictions))\n",
        "  print(\"ACTUAL: \" + str(actual))\n",
        "\n",
        "  # Compare synonym lists to actual\n",
        "  match_found = False\n",
        "  for word, synonyms in zip(actual, synonym_lists):\n",
        "    print()\n",
        "    print(word)\n",
        "    print(synonyms)\n",
        "    if word in synonyms:\n",
        "      match_found = True\n",
        "    else:\n",
        "      match_found = False\n",
        "    scores.append(match_found)\n",
        "\n",
        "  # Calculate and return metric\n",
        "  return sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "IbkCiprDvx3E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_synonym_pipeline_rename_me(predictions, actual, 'en', 'es')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EQctYCU8YXy",
        "outputId": "b9dfd0ac-f198-40fc-aab9-2bb6785a283a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "PREDICTIONS: ['pizza', 'turkey', 'baño', 'book', 'manzana', 'tiger']\n",
            "ACTUAL: ['whale', 'chicken', 'bathroom', 'libro', 'plátano', 'tiger']\n",
            "\n",
            "whale\n",
            "['pizza', 'Pizza', 'pizza_pie']\n",
            "\n",
            "chicken\n",
            "['turkey', 'pavo', 'Meleagris_gallopavo', 'Turkey', 'Republic_of_Turkey', 'joker', 'bomb', 'dud', 'Pavo', 'género_Pavo', 'Pavo']\n",
            "\n",
            "bathroom\n",
            "['baño', 'bathroom', 'aplicación', 'capa', 'cobertura', 'mano', 'revestimiento', 'bañera', 'capa', 'mano', 'recubrimiento', 'inodoro', 'bath', 'toilet', 'lavatory', 'lav', 'can', 'john', 'privy']\n",
            "\n",
            "libro\n",
            "['book', 'libro', 'volume', 'record', 'record_book', 'script', 'playscript', 'ledger', 'leger', 'account_book', 'book_of_account', 'rule_book', 'Koran', 'Quran', \"al-Qur'an\", 'Book', 'Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word', 'reserve', 'hold', 'ejemplar', 'volumen', 'Biblia', 'Biblia_cristiana', 'Libro', 'biblia', 'el_buen_libro', 'la_palabra', 'las_escrituras', 'las_sagradas_escrituras', 'libro_mayor']\n",
            "\n",
            "plátano\n",
            "['manzana', 'Manzana', 'manzana', 'manzana']\n",
            "\n",
            "tiger\n",
            "['tiger', 'tigre', 'Panthera_tigris']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}