{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOBZNuN9nfBF7UnlLsuHh/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnbpdx/code_switched_next_word_predictor/blob/main/synonym_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps, from [Refined Project Proposal](https://docs.google.com/document/d/1NRUdfsiXkgPQW7Mg-CSVs8FDzghQjYWcvZ9JPUFfLW4/edit?usp=sharing)\n",
        "\n",
        "1. Pass the predicted next word to a pre-trained language recognition model to **determine the language** of the word (one of the two languages in our mixed, code-switched corpus.)\n",
        "2. **Translate** the predicted next word into the other language by passing it into an appropriate translation model.\n",
        "3. **Get n synonyms** of predicted next word in both languages using cosine distance between word embeddings, gathered from two vector embedding models, one in each language. This could also be done by a GPT-3 model.\n",
        "4. **Score the model** based on whether or not the actual next word is in the list of predicted next word bilingual synonyms.\n"
      ],
      "metadata": {
        "id": "leY02gxZwIZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Synonym Pipeline"
      ],
      "metadata": {
        "id": "QR8zDeBpu6vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports/Downloads"
      ],
      "metadata": {
        "id": "vSoAcqdvvZjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U easynmt\n",
        "!pip install sacremoses # was told to install this by a warning\n",
        "\n",
        "from easynmt import EasyNMT\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "QFXklFVVvbl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373a67a9-259d-4f81-90a0-68112e13ac29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting easynmt\n",
            "  Downloading EasyNMT-2.0.2.tar.gz (23 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.64.1)\n",
            "Collecting transformers<5,>=4.4\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 65.8 MB/s \n",
            "\u001b[?25hCollecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->easynmt) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 80.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (4.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5,>=4.4->easynmt) (3.0.9)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=4.4->easynmt) (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.24.3)\n",
            "Building wheels for collected packages: easynmt, fasttext\n",
            "  Building wheel for easynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easynmt: filename=EasyNMT-2.0.2-py3-none-any.whl size=19917 sha256=04e41f7a2fa70b879695d2bd3d72fb5543e6395ad22bb7b7a7e30ea8525c4b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/57/06/53ca38645e14d4537a41e5a36da2026e10f54ae88240dd5190\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3164590 sha256=7075495894883788334031cce2977eeb64bb5fad02e2232629cadce661e0fd66\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built easynmt fasttext\n",
            "Installing collected packages: tokenizers, pybind11, huggingface-hub, transformers, sentencepiece, fasttext, easynmt\n",
            "Successfully installed easynmt-2.0.2 fasttext-0.9.2 huggingface-hub-0.11.0 pybind11-2.10.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=3bf53f5a46a23d14a01a8fbf448c75380264ed2e05dc06d7cf707d1fe1c579cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fake Data"
      ],
      "metadata": {
        "id": "_WOsNzR3viK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake Data\n",
        "predictions = ['pizza', 'turkey', 'baño', 'book', 'manzana', 'tiger']\n",
        "actual =    ['whale', 'chicken', 'bathroom', 'libro', 'plátano', 'tiger']"
      ],
      "metadata": {
        "id": "m1BrAIG8u95-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "tUbm1Gyf4SGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I think I should be raising the exceptions here and catching them in\n",
        "# the pipeline function actually, but i dont wanna waste time on that rn. \n",
        "def langs_are_valid(lang1, lang2):\n",
        "  if lang1 == lang2:\n",
        "    print('Langugages must not be the same.')\n",
        "    return False\n",
        "  \n",
        "  if lang1 != 'en' and lang1 != 'es':\n",
        "    print('Langugage must be either Spanish or English.')\n",
        "    return False\n",
        "  \n",
        "  if lang2 != 'en' and lang2 != 'es':\n",
        "    print('Langugage must be either Spanish or English.')\n",
        "    return False\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "Ba-4kOKv4T6P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "IwC7UnjtvjY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- CITE ME -- Look at Mark's scratchpad noteboook for everything we gotta cite\n",
        "# -- POSSIBLE ISSUE -- 2x check my old comments for issues\n",
        "# -- POSSIBLE ISSUE -- -- DISCUSSION -- When I run id_model.language_detection on \"manzana\" it returns 'en'.\n",
        "# \"manzana\" IS a word in English, that means like a unit of land or something. That\n",
        "# wouldn't be a problem (hey, maybe the person speaking Spanglish here DID mean \"a unit of land\")\n",
        "# but in this case, it's SO obvious that there's another, much more common Spanish word\n",
        "# that means \"manzana\". I feel like the ID model should have caught that, by comparing\n",
        "# the frequency of the English and Spanish versions. My guess is that this model is\n",
        "# more used to translating sentences in context rather than words. We can sub it out\n",
        "# with a different language detection/identification model later then, if we notice this problem at a large scale.\n",
        "# That shouldn't be hard, there's many. I also wonder if what's happening here is that\n",
        "# the model is capable of interpreting semi-code-switched sentences and translating them.\n",
        "# It has to ID them first, so it would interpret an English sentence with a few Spanish words\n",
        "# as \"English\", then do the translation step. For that to work, it'd have to have some sense\n",
        "# of \"manzana\" as an English word. IOW, could be a NN problem, or a problem of a model that\n",
        "# is intended to do translation too. Same easy solution though, just sub it out. We\n",
        "# could even sub it out with an API call to Dictionary.com or something, doesn't\n",
        "# have to be a NN model.\n",
        "# Okay now I'm getting different languages detected than English or Spanish. Which\n",
        "# is a tougher problem to solve. We can switch out the model, but idk if I'm gonna\n",
        "# be able to find a model that allows us to specify: \"Only return one of these\n",
        "# two languages\". The big question is, what do we when our detection model returns\n",
        "# null, or returns an unknown language? Throw out the prediction from the score?\n",
        "\n",
        "# -- TODO -- Get rid of all the print statements\n",
        "# -- TODO -- Make the comments better\n",
        "\n",
        "\n",
        "# -- TODO -- Right now, this function only ACTUALLY works for Spanish and English, gotta\n",
        "# figure out how to convert from the EasyNMT language strings to the WordNet\n",
        "# strings. Or I could just hack it. I'm not 100% sure yet that WN works for \n",
        "# Mandarin anyway, which would be an issue.\n",
        "def score_with_synonym_list(predictions, actual, lang1, lang2):\n",
        "  if not langs_are_valid(lang1, lang2):\n",
        "    raise Exception('Please use valid EasyNMT language strings.') # idk how to use exceptions in python, someone tell me if this is wrong\n",
        "\n",
        "  id_model = EasyNMT('opus-mt')\n",
        "  translation_model = EasyNMT('opus-mt')\n",
        "\n",
        "  synonym_lists = []\n",
        "  scores = []\n",
        "\n",
        "  for word in predictions:\n",
        "\n",
        "    # Determine the language of word\n",
        "    detected_lang = id_model.language_detection(word)\n",
        "    if (detected_lang == lang1):\n",
        "      other_lang = lang2\n",
        "    elif (detected_lang == lang2): \n",
        "      other_lang = lang1\n",
        "    else:\n",
        "      print('EasyNMT detected a language other than lang1 or lang2. Detected Language -> ' + detected_lang)\n",
        "      raise Exception\n",
        "\n",
        "    # Translate word into the OTHER language\n",
        "    translated_word = translation_model.translate(word, source_lang=detected_lang, target_lang=other_lang) # or swap the languages for the reverse, obv. \n",
        "\n",
        "    if (detected_lang == 'en'): # need to update this step to make this function generic with respect to languages\n",
        "      detected_lang = 'eng'\n",
        "      other_lang = 'spa'\n",
        "    else:\n",
        "      detected_lang = 'spa'\n",
        "      other_lang = 'eng'\n",
        "\n",
        "    # -- POSSIBLE ISSUE -- Should probably limit the number of words, right now there's roughly 0 - 20 for each.\n",
        "    # Get synonym lists for word and translated_word\n",
        "\n",
        "    # -- POSSIBLE ISSUE -- I'm combining the word and translated_word synonyms, we can split later if needed\n",
        "    synonyms = []\n",
        "\n",
        "    # add the words themselves to the synonym list\n",
        "    synonyms.append(word)\n",
        "    synonyms.append(translated_word)\n",
        "\n",
        "    # add synonyms of word\n",
        "    for synonym in wn.synsets(word, lang=detected_lang):\n",
        "      for item in synonym.lemmas(detected_lang):\n",
        "          if item.name() != word:\n",
        "            synonyms.append(item.name())\n",
        "\n",
        "    # add synonyms of translated_word\n",
        "    for synonym in wn.synsets(translated_word, lang=other_lang):\n",
        "      for item in synonym.lemmas(other_lang):\n",
        "          if item.name() != translated_word:\n",
        "            synonyms.append(item.name())\n",
        "\n",
        "    # Throw all the discovered synonyms (and word & translated_word) onto synonym_lists\n",
        "    synonym_lists.append(synonyms)\n",
        "\n",
        "  # print()\n",
        "  # print()\n",
        "  # print(\"PREDICTIONS: \" + str(predictions))\n",
        "  # print(\"ACTUAL: \" + str(actual))\n",
        "\n",
        "  # Compare synonym lists to actual\n",
        "  match_found = False\n",
        "  for word, synonyms in zip(actual, synonym_lists):\n",
        "    # print()\n",
        "    # print(word)\n",
        "    # print(synonyms)\n",
        "    if word in synonyms:\n",
        "      match_found = True\n",
        "    else:\n",
        "      match_found = False\n",
        "    scores.append(match_found)\n",
        "\n",
        "  # Calculate and return metric\n",
        "  return sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "IbkCiprDvx3E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just the basic metric, iow, whether or not the predicted word is equal\n",
        "# to the actual next word.\n",
        "def score_with_actual_word(predictions, actual):\n",
        "  scores = []\n",
        "\n",
        "  for prediction, actual in zip(predictions, actual):\n",
        "    if prediction == actual:\n",
        "      scores.append(True)\n",
        "    else:\n",
        "      scores.append(False)\n",
        "\n",
        "  return sum(scores) / len(scores)\n"
      ],
      "metadata": {
        "id": "bfNaP3oUxP1t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_with_synonym_list(predictions, actual, 'en', 'es')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EQctYCU8YXy",
        "outputId": "bbaea429-bb68-4332-bfdd-1730d93c07eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_with_actual_word(predictions, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvyKAuycdr",
        "outputId": "82c45487-7bad-4022-efd2-876ffca376d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}